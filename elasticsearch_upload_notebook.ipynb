{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elasticsearch configuration\n",
    "ES_HOST = \"http://localhost:9200\"\n",
    "INDEX_NAME = \"articles_content_title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticsearchUploader:\n",
    "    def __init__(self, es_host=ES_HOST):\n",
    "        self.es_host = es_host\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({'Content-Type': 'application/json'})\n",
    "        \n",
    "    def test_connection(self):\n",
    "        \"\"\"Test káº¿t ná»‘i Elasticsearch\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.es_host}\")\n",
    "            if response.status_code == 200:\n",
    "                cluster_info = response.json()\n",
    "                print(f\"âœ… Connected to Elasticsearch {cluster_info['version']['number']}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"âŒ Connection failed: {response.status_code}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Connection error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_index(self, index_name=INDEX_NAME):\n",
    "        \"\"\"Táº¡o index vá»›i mapping\"\"\"\n",
    "        \n",
    "        # XÃ³a index cÅ© náº¿u tá»“n táº¡i\n",
    "        self.session.delete(f\"{self.es_host}/{index_name}\")\n",
    "        \n",
    "        mapping = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"article_id\": {\"type\": \"keyword\"},\n",
    "                    \"url\": {\"type\": \"keyword\"},\n",
    "                    \"title\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"standard\",\n",
    "                        \"fields\": {\n",
    "                            \"raw\": {\"type\": \"keyword\"},\n",
    "                            \"suggest\": {\"type\": \"completion\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"content\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"standard\"\n",
    "                    },\n",
    "                    \"date\": {\n",
    "                        \"type\": \"date\",\n",
    "                        \"format\": \"yyyy-MM-dd'T'HH:mm:ss'Z'||yyyy-MM-dd||epoch_millis\"\n",
    "                    },\n",
    "                    \"images\": {\"type\": \"keyword\"},\n",
    "                    \"created_at\": {\"type\": \"date\"},\n",
    "                    \"updated_at\": {\"type\": \"date\"}\n",
    "                }\n",
    "            },\n",
    "            \"settings\": {\n",
    "                \"number_of_shards\": 2,\n",
    "                \"number_of_replicas\": 1,\n",
    "                \"refresh_interval\": \"30s\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.put(\n",
    "                f\"{self.es_host}/{index_name}\",\n",
    "                data=json.dumps(mapping)\n",
    "            )\n",
    "            \n",
    "            if response.status_code in [200, 201]:\n",
    "                print(f\"âœ… Index '{index_name}' created successfully\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"âŒ Failed to create index: {response.text}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error creating index: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def bulk_upload(self, documents):\n",
    "        \"\"\"Upload documents báº±ng bulk API\"\"\"\n",
    "        \n",
    "        # Táº¡o bulk request body\n",
    "        bulk_body = \"\"\n",
    "        for doc in documents:\n",
    "            # Action line\n",
    "            action = {\n",
    "                \"index\": {\n",
    "                    \"_index\": doc[\"_index\"],\n",
    "                    \"_id\": doc[\"_id\"]\n",
    "                }\n",
    "            }\n",
    "            bulk_body += json.dumps(action) + \"\\n\"\n",
    "            \n",
    "            # Document line\n",
    "            bulk_body += json.dumps(doc[\"_source\"]) + \"\\n\"\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(\n",
    "                f\"{self.es_host}/_bulk\",\n",
    "                data=bulk_body,\n",
    "                headers={'Content-Type': 'application/x-ndjson'},\n",
    "                timeout=120\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                \n",
    "                # Äáº¿m success vÃ  errors\n",
    "                success_count = 0\n",
    "                errors = []\n",
    "                \n",
    "                for item in result.get('items', []):\n",
    "                    if 'index' in item:\n",
    "                        if item['index'].get('status') in [200, 201]:\n",
    "                            success_count += 1\n",
    "                        else:\n",
    "                            errors.append(item['index'])\n",
    "                \n",
    "                return success_count, errors\n",
    "            else:\n",
    "                print(f\"âŒ Bulk upload failed: {response.status_code} - {response.text}\")\n",
    "                return 0, documents\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Bulk upload error: {e}\")\n",
    "            return 0, documents\n",
    "    \n",
    "    def search(self, query, size=10):\n",
    "        \"\"\"Search documents\"\"\"\n",
    "        search_body = {\n",
    "            \"query\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": query,\n",
    "                    \"fields\": [\"title^3\", \"content^1\"],\n",
    "                    \"type\": \"best_fields\",\n",
    "                    \"fuzziness\": \"AUTO\"\n",
    "                }\n",
    "            },\n",
    "            \"highlight\": {\n",
    "                \"fields\": {\n",
    "                    \"title\": {},\n",
    "                    \"content\": {\n",
    "                        \"fragment_size\": 150,\n",
    "                        \"number_of_fragments\": 3\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"size\": size\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(\n",
    "                f\"{self.es_host}/{INDEX_NAME}/_search\",\n",
    "                data=json.dumps(search_body)\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                print(f\"âŒ Search failed: {response.text}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Search error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_index_stats(self, index_name=INDEX_NAME):\n",
    "        \"\"\"Láº¥y thá»‘ng kÃª index\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.es_host}/{index_name}/_stats\")\n",
    "            if response.status_code == 200:\n",
    "                stats = response.json()\n",
    "                doc_count = stats['indices'][index_name]['total']['docs']['count']\n",
    "                size_bytes = stats['indices'][index_name]['total']['store']['size_in_bytes']\n",
    "                return doc_count, size_bytes\n",
    "            else:\n",
    "                return 0, 0\n",
    "        except:\n",
    "            return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_json_and_upload(json_file_path, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Stream JSON file vÃ  upload báº±ng requests\n",
    "    \"\"\"\n",
    "    \n",
    "    # Khá»Ÿi táº¡o uploader\n",
    "    uploader = ElasticsearchUploader()\n",
    "    \n",
    "    # Test connection\n",
    "    if not uploader.test_connection():\n",
    "        return False\n",
    "    \n",
    "    # Create index\n",
    "    if not uploader.create_index():\n",
    "        return False\n",
    "    \n",
    "    print(f\"ğŸ“ Processing file: {json_file_path}\")\n",
    "    print(f\"ğŸ“ File size: {os.path.getsize(json_file_path) / (1024**3):.2f} GB\")\n",
    "    print(f\"ğŸ”¢ Batch size: {batch_size}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Counters\n",
    "    total_processed = 0\n",
    "    total_success = 0\n",
    "    total_errors = 0\n",
    "    batch = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        with open(json_file_path, 'rb') as file:\n",
    "            parser = ijson.kvitems(file, '')\n",
    "            \n",
    "            for article_id, article_data in parser:\n",
    "                try:\n",
    "                    # Validate\n",
    "                    if not isinstance(article_data, dict):\n",
    "                        continue\n",
    "                    \n",
    "                    # Create document\n",
    "                    doc = {\n",
    "                        \"_index\": INDEX_NAME,\n",
    "                        \"_id\": article_id,\n",
    "                        \"_source\": {\n",
    "                            \"article_id\": article_id,\n",
    "                            \"url\": article_data.get(\"url\", \"\"),\n",
    "                            \"title\": article_data.get(\"title\", \"\"),\n",
    "                            \"content\": article_data.get(\"content\", \"\"),\n",
    "                            \"date\": article_data.get(\"date\"),\n",
    "                            \"images\": article_data.get(\"images\", []),\n",
    "                            \"created_at\": datetime.now().isoformat(),\n",
    "                            \"updated_at\": datetime.now().isoformat()\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    batch.append(doc)\n",
    "                    \n",
    "                    # Upload batch khi Ä‘á»§\n",
    "                    if len(batch) >= batch_size:\n",
    "                        success_count, errors = uploader.bulk_upload(batch)\n",
    "                        \n",
    "                        total_processed += len(batch)\n",
    "                        total_success += success_count\n",
    "                        total_errors += len(errors) if errors else 0\n",
    "                        \n",
    "                        # Progress update\n",
    "                        elapsed = time.time() - start_time\n",
    "                        speed = total_processed / elapsed if elapsed > 0 else 0\n",
    "                        \n",
    "                        print(f\"ğŸ“Š Processed: {total_processed:,} | âœ… {total_success:,} | âŒ {total_errors} | âš¡ {speed:.0f} docs/sec\")\n",
    "                        \n",
    "                        if errors:\n",
    "                            print(f\"   âš ï¸ Last batch errors: {len(errors)}\")\n",
    "                        \n",
    "                        batch = []  # Reset batch\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Error processing {article_id}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Upload batch cuá»‘i\n",
    "            if batch:\n",
    "                success_count, errors = uploader.bulk_upload(batch)\n",
    "                total_processed += len(batch)\n",
    "                total_success += success_count\n",
    "                total_errors += len(errors) if errors else 0\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading file: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Final stats\n",
    "    elapsed = time.time() - start_time\n",
    "    avg_speed = total_processed / elapsed if elapsed > 0 else 0\n",
    "    success_rate = (total_success / total_processed * 100) if total_processed > 0 else 0\n",
    "    \n",
    "    print(f\"\\nğŸ‰ UPLOAD COMPLETED!\")\n",
    "    print(f\"â±ï¸  Total time: {elapsed/60:.1f} minutes\")\n",
    "    print(f\"ğŸ“„ Documents processed: {total_processed:,}\")\n",
    "    print(f\"âœ… Successful: {total_success:,} ({success_rate:.1f}%)\")\n",
    "    print(f\"âŒ Errors: {total_errors}\")\n",
    "    print(f\"âš¡ Average speed: {avg_speed:.0f} docs/sec\")\n",
    "    \n",
    "    # Refresh index\n",
    "    try:\n",
    "        uploader.session.post(f\"{uploader.es_host}/{INDEX_NAME}/_refresh\")\n",
    "        print(\"ğŸ”„ Index refreshed\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Verify upload\n",
    "    doc_count, size_bytes = uploader.get_index_stats()\n",
    "    print(f\"ğŸ“Š Final index stats: {doc_count:,} docs, {size_bytes/(1024**2):.1f} MB\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def test_search_functionality():\n",
    "    \"\"\"Test search sau khi upload\"\"\"\n",
    "    uploader = ElasticsearchUploader()\n",
    "    \n",
    "    print(\"\\nğŸ§ª TESTING SEARCH\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Test search\n",
    "    results = uploader.search(\"TuSimple autonomous truck\", size=3)\n",
    "    \n",
    "    if results and results['hits']['total']['value'] > 0:\n",
    "        print(f\"âœ… Found {results['hits']['total']['value']} results\")\n",
    "        \n",
    "        for i, hit in enumerate(results['hits']['hits'], 1):\n",
    "            source = hit['_source']\n",
    "            print(f\"\\n{i}. {source['title']}\")\n",
    "            print(f\"   Score: {hit['_score']:.2f}\")\n",
    "            print(f\"   URL: {source['url']}\")\n",
    "            \n",
    "            # Highlight\n",
    "            if 'highlight' in hit and 'content' in hit['highlight']:\n",
    "                print(f\"   Preview: {hit['highlight']['content'][0]}...\")\n",
    "    else:\n",
    "        print(\"âŒ No search results found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ELASTICSEARCH UPLOADER WITH REQUESTS\n",
      "==================================================\n",
      "ğŸ“ File: database_article_to_alls.json (1.01 GB)\n",
      "âœ… Connected to Elasticsearch 9.0.1\n",
      "âœ… Index 'articles_content_title' created successfully\n",
      "ğŸ“ Processing file: database_article_to_alls.json\n",
      "ğŸ“ File size: 1.01 GB\n",
      "ğŸ”¢ Batch size: 2000\n",
      "--------------------------------------------------\n",
      "ğŸ“Š Processed: 2,000 | âœ… 2,000 | âŒ 0 | âš¡ 427 docs/sec\n",
      "ğŸ“Š Processed: 4,000 | âœ… 4,000 | âŒ 0 | âš¡ 448 docs/sec\n",
      "ğŸ“Š Processed: 6,000 | âœ… 6,000 | âŒ 0 | âš¡ 434 docs/sec\n",
      "ğŸ“Š Processed: 8,000 | âœ… 8,000 | âŒ 0 | âš¡ 495 docs/sec\n",
      "ğŸ“Š Processed: 10,000 | âœ… 10,000 | âŒ 0 | âš¡ 487 docs/sec\n",
      "ğŸ“Š Processed: 12,000 | âœ… 12,000 | âŒ 0 | âš¡ 503 docs/sec\n",
      "ğŸ“Š Processed: 14,000 | âœ… 14,000 | âŒ 0 | âš¡ 503 docs/sec\n",
      "ğŸ“Š Processed: 16,000 | âœ… 16,000 | âŒ 0 | âš¡ 487 docs/sec\n",
      "ğŸ“Š Processed: 18,000 | âœ… 18,000 | âŒ 0 | âš¡ 493 docs/sec\n",
      "ğŸ“Š Processed: 20,000 | âœ… 20,000 | âŒ 0 | âš¡ 495 docs/sec\n",
      "ğŸ“Š Processed: 22,000 | âœ… 22,000 | âŒ 0 | âš¡ 509 docs/sec\n",
      "ğŸ“Š Processed: 24,000 | âœ… 24,000 | âŒ 0 | âš¡ 518 docs/sec\n",
      "ğŸ“Š Processed: 26,000 | âœ… 26,000 | âŒ 0 | âš¡ 527 docs/sec\n",
      "ğŸ“Š Processed: 28,000 | âœ… 28,000 | âŒ 0 | âš¡ 533 docs/sec\n",
      "ğŸ“Š Processed: 30,000 | âœ… 30,000 | âŒ 0 | âš¡ 539 docs/sec\n",
      "ğŸ“Š Processed: 32,000 | âœ… 32,000 | âŒ 0 | âš¡ 547 docs/sec\n",
      "ğŸ“Š Processed: 34,000 | âœ… 34,000 | âŒ 0 | âš¡ 556 docs/sec\n",
      "ğŸ“Š Processed: 36,000 | âœ… 36,000 | âŒ 0 | âš¡ 560 docs/sec\n",
      "ğŸ“Š Processed: 38,000 | âœ… 38,000 | âŒ 0 | âš¡ 563 docs/sec\n",
      "ğŸ“Š Processed: 40,000 | âœ… 40,000 | âŒ 0 | âš¡ 565 docs/sec\n",
      "ğŸ“Š Processed: 42,000 | âœ… 42,000 | âŒ 0 | âš¡ 570 docs/sec\n",
      "ğŸ“Š Processed: 44,000 | âœ… 44,000 | âŒ 0 | âš¡ 574 docs/sec\n",
      "ğŸ“Š Processed: 46,000 | âœ… 46,000 | âŒ 0 | âš¡ 579 docs/sec\n",
      "ğŸ“Š Processed: 48,000 | âœ… 48,000 | âŒ 0 | âš¡ 582 docs/sec\n",
      "ğŸ“Š Processed: 50,000 | âœ… 50,000 | âŒ 0 | âš¡ 584 docs/sec\n",
      "ğŸ“Š Processed: 52,000 | âœ… 52,000 | âŒ 0 | âš¡ 581 docs/sec\n",
      "ğŸ“Š Processed: 54,000 | âœ… 54,000 | âŒ 0 | âš¡ 579 docs/sec\n",
      "ğŸ“Š Processed: 56,000 | âœ… 56,000 | âŒ 0 | âš¡ 574 docs/sec\n",
      "ğŸ“Š Processed: 58,000 | âœ… 58,000 | âŒ 0 | âš¡ 567 docs/sec\n",
      "ğŸ“Š Processed: 60,000 | âœ… 60,000 | âŒ 0 | âš¡ 565 docs/sec\n",
      "ğŸ“Š Processed: 62,000 | âœ… 62,000 | âŒ 0 | âš¡ 564 docs/sec\n",
      "ğŸ“Š Processed: 64,000 | âœ… 64,000 | âŒ 0 | âš¡ 566 docs/sec\n",
      "ğŸ“Š Processed: 66,000 | âœ… 66,000 | âŒ 0 | âš¡ 570 docs/sec\n",
      "ğŸ“Š Processed: 68,000 | âœ… 68,000 | âŒ 0 | âš¡ 569 docs/sec\n",
      "ğŸ“Š Processed: 70,000 | âœ… 70,000 | âŒ 0 | âš¡ 566 docs/sec\n",
      "ğŸ“Š Processed: 72,000 | âœ… 72,000 | âŒ 0 | âš¡ 563 docs/sec\n",
      "ğŸ“Š Processed: 74,000 | âœ… 74,000 | âŒ 0 | âš¡ 557 docs/sec\n",
      "ğŸ“Š Processed: 76,000 | âœ… 76,000 | âŒ 0 | âš¡ 553 docs/sec\n",
      "ğŸ“Š Processed: 78,000 | âœ… 78,000 | âŒ 0 | âš¡ 554 docs/sec\n",
      "ğŸ“Š Processed: 80,000 | âœ… 80,000 | âŒ 0 | âš¡ 554 docs/sec\n",
      "ğŸ“Š Processed: 82,000 | âœ… 82,000 | âŒ 0 | âš¡ 556 docs/sec\n",
      "ğŸ“Š Processed: 84,000 | âœ… 84,000 | âŒ 0 | âš¡ 557 docs/sec\n",
      "ğŸ“Š Processed: 86,000 | âœ… 86,000 | âŒ 0 | âš¡ 560 docs/sec\n",
      "ğŸ“Š Processed: 88,000 | âœ… 88,000 | âŒ 0 | âš¡ 562 docs/sec\n",
      "ğŸ“Š Processed: 90,000 | âœ… 90,000 | âŒ 0 | âš¡ 563 docs/sec\n",
      "ğŸ“Š Processed: 92,000 | âœ… 92,000 | âŒ 0 | âš¡ 562 docs/sec\n",
      "ğŸ“Š Processed: 94,000 | âœ… 94,000 | âŒ 0 | âš¡ 565 docs/sec\n",
      "ğŸ“Š Processed: 96,000 | âœ… 96,000 | âŒ 0 | âš¡ 564 docs/sec\n",
      "ğŸ“Š Processed: 98,000 | âœ… 98,000 | âŒ 0 | âš¡ 565 docs/sec\n",
      "ğŸ“Š Processed: 100,000 | âœ… 100,000 | âŒ 0 | âš¡ 565 docs/sec\n",
      "ğŸ“Š Processed: 102,000 | âœ… 102,000 | âŒ 0 | âš¡ 564 docs/sec\n",
      "ğŸ“Š Processed: 104,000 | âœ… 104,000 | âŒ 0 | âš¡ 563 docs/sec\n",
      "ğŸ“Š Processed: 106,000 | âœ… 106,000 | âŒ 0 | âš¡ 564 docs/sec\n",
      "ğŸ“Š Processed: 108,000 | âœ… 108,000 | âŒ 0 | âš¡ 566 docs/sec\n",
      "ğŸ“Š Processed: 110,000 | âœ… 110,000 | âŒ 0 | âš¡ 569 docs/sec\n",
      "ğŸ“Š Processed: 112,000 | âœ… 112,000 | âŒ 0 | âš¡ 570 docs/sec\n",
      "ğŸ“Š Processed: 114,000 | âœ… 114,000 | âŒ 0 | âš¡ 570 docs/sec\n",
      "ğŸ“Š Processed: 116,000 | âœ… 116,000 | âŒ 0 | âš¡ 570 docs/sec\n",
      "ğŸ“Š Processed: 118,000 | âœ… 118,000 | âŒ 0 | âš¡ 572 docs/sec\n",
      "ğŸ“Š Processed: 120,000 | âœ… 120,000 | âŒ 0 | âš¡ 574 docs/sec\n",
      "ğŸ“Š Processed: 122,000 | âœ… 122,000 | âŒ 0 | âš¡ 576 docs/sec\n",
      "ğŸ“Š Processed: 124,000 | âœ… 124,000 | âŒ 0 | âš¡ 576 docs/sec\n",
      "ğŸ“Š Processed: 126,000 | âœ… 126,000 | âŒ 0 | âš¡ 576 docs/sec\n",
      "ğŸ“Š Processed: 128,000 | âœ… 128,000 | âŒ 0 | âš¡ 576 docs/sec\n",
      "ğŸ“Š Processed: 130,000 | âœ… 130,000 | âŒ 0 | âš¡ 577 docs/sec\n",
      "ğŸ“Š Processed: 132,000 | âœ… 132,000 | âŒ 0 | âš¡ 579 docs/sec\n",
      "ğŸ“Š Processed: 134,000 | âœ… 134,000 | âŒ 0 | âš¡ 581 docs/sec\n",
      "ğŸ“Š Processed: 136,000 | âœ… 136,000 | âŒ 0 | âš¡ 583 docs/sec\n",
      "ğŸ“Š Processed: 138,000 | âœ… 138,000 | âŒ 0 | âš¡ 584 docs/sec\n",
      "ğŸ“Š Processed: 140,000 | âœ… 140,000 | âŒ 0 | âš¡ 585 docs/sec\n",
      "ğŸ“Š Processed: 142,000 | âœ… 142,000 | âŒ 0 | âš¡ 585 docs/sec\n",
      "ğŸ“Š Processed: 144,000 | âœ… 144,000 | âŒ 0 | âš¡ 585 docs/sec\n",
      "ğŸ“Š Processed: 146,000 | âœ… 146,000 | âŒ 0 | âš¡ 587 docs/sec\n",
      "ğŸ“Š Processed: 148,000 | âœ… 148,000 | âŒ 0 | âš¡ 588 docs/sec\n",
      "ğŸ“Š Processed: 150,000 | âœ… 150,000 | âŒ 0 | âš¡ 588 docs/sec\n",
      "ğŸ“Š Processed: 152,000 | âœ… 152,000 | âŒ 0 | âš¡ 588 docs/sec\n",
      "ğŸ“Š Processed: 154,000 | âœ… 154,000 | âŒ 0 | âš¡ 589 docs/sec\n",
      "ğŸ“Š Processed: 156,000 | âœ… 156,000 | âŒ 0 | âš¡ 589 docs/sec\n",
      "ğŸ“Š Processed: 158,000 | âœ… 158,000 | âŒ 0 | âš¡ 588 docs/sec\n",
      "ğŸ“Š Processed: 160,000 | âœ… 160,000 | âŒ 0 | âš¡ 588 docs/sec\n",
      "ğŸ“Š Processed: 162,000 | âœ… 162,000 | âŒ 0 | âš¡ 587 docs/sec\n",
      "ğŸ“Š Processed: 164,000 | âœ… 164,000 | âŒ 0 | âš¡ 586 docs/sec\n",
      "ğŸ“Š Processed: 166,000 | âœ… 166,000 | âŒ 0 | âš¡ 586 docs/sec\n",
      "ğŸ“Š Processed: 168,000 | âœ… 168,000 | âŒ 0 | âš¡ 583 docs/sec\n",
      "ğŸ“Š Processed: 170,000 | âœ… 170,000 | âŒ 0 | âš¡ 581 docs/sec\n",
      "ğŸ“Š Processed: 172,000 | âœ… 172,000 | âŒ 0 | âš¡ 581 docs/sec\n",
      "ğŸ“Š Processed: 174,000 | âœ… 174,000 | âŒ 0 | âš¡ 582 docs/sec\n",
      "ğŸ“Š Processed: 176,000 | âœ… 176,000 | âŒ 0 | âš¡ 581 docs/sec\n",
      "ğŸ“Š Processed: 178,000 | âœ… 178,000 | âŒ 0 | âš¡ 581 docs/sec\n",
      "ğŸ“Š Processed: 180,000 | âœ… 180,000 | âŒ 0 | âš¡ 581 docs/sec\n",
      "ğŸ“Š Processed: 182,000 | âœ… 182,000 | âŒ 0 | âš¡ 581 docs/sec\n",
      "ğŸ“Š Processed: 184,000 | âœ… 184,000 | âŒ 0 | âš¡ 582 docs/sec\n",
      "ğŸ“Š Processed: 186,000 | âœ… 186,000 | âŒ 0 | âš¡ 581 docs/sec\n",
      "ğŸ“Š Processed: 188,000 | âœ… 188,000 | âŒ 0 | âš¡ 580 docs/sec\n",
      "ğŸ“Š Processed: 190,000 | âœ… 190,000 | âŒ 0 | âš¡ 578 docs/sec\n",
      "ğŸ“Š Processed: 192,000 | âœ… 192,000 | âŒ 0 | âš¡ 577 docs/sec\n",
      "ğŸ“Š Processed: 194,000 | âœ… 194,000 | âŒ 0 | âš¡ 577 docs/sec\n",
      "ğŸ“Š Processed: 196,000 | âœ… 196,000 | âŒ 0 | âš¡ 577 docs/sec\n",
      "ğŸ“Š Processed: 198,000 | âœ… 198,000 | âŒ 0 | âš¡ 578 docs/sec\n",
      "ğŸ“Š Processed: 200,000 | âœ… 200,000 | âŒ 0 | âš¡ 579 docs/sec\n",
      "ğŸ“Š Processed: 202,000 | âœ… 202,000 | âŒ 0 | âš¡ 580 docs/sec\n",
      "\n",
      "ğŸ‰ UPLOAD COMPLETED!\n",
      "â±ï¸  Total time: 5.8 minutes\n",
      "ğŸ“„ Documents processed: 202,803\n",
      "âœ… Successful: 202,803 (100.0%)\n",
      "âŒ Errors: 0\n",
      "âš¡ Average speed: 582 docs/sec\n",
      "ğŸ”„ Index refreshed\n",
      "ğŸ“Š Final index stats: 202,803 docs, 1170.2 MB\n",
      "\n",
      "ğŸ§ª TESTING SEARCH\n",
      "------------------------------\n",
      "âœ… Found 10000 results\n",
      "\n",
      "1. Roboracer autonomous car to make history at Goodwood\n",
      "   Score: 34.34\n",
      "   URL: https://www.cnn.com/2018/06/28/sport/roboracer-goodwood-festival-of-speed-spt-intl/index.html\n",
      "   Preview: It is a venerable shrine to the thrills of the internal combustion engine, but Goodwood is famous Festival of Speed will welcome an <em>autonomous</em> electric...\n",
      "\n",
      "2. AIâ€™s â€˜Oppenheimer momentâ€™: autonomous weapons enter the battlefield\n",
      "   Score: 34.34\n",
      "   URL: https://www.theguardian.com/technology/article/2024/jul/14/ais-oppenheimer-moment-autonomous-weapons-enter-the-battlefield\n",
      "   Preview: One of them makes a call over his radio, and within moments a fleet of small <em>autonomous</em> drones equipped with explosives fly through the town square, entering...\n",
      "\n",
      "3. Robot surgeons provide many benefits, but how autonomous should they be?\n",
      "   Score: 33.98\n",
      "   URL: https://www.theguardian.com/science/2023/jun/18/robot-surgeons-provide-many-benefits-but-how-autonomous-should-they-be\n",
      "   Preview: In what they described as one of the most delicate procedures in the practice of surgery, their Smart Tissue <em>Autonomous</em> Robot (Star) sutured the ends of...\n",
      "\n",
      "âœ… All done!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ ELASTICSEARCH UPLOADER WITH REQUESTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Configuration\n",
    "    json_file = \"database_article_to_alls.json\"  # Thay Ä‘Æ°á»ng dáº«n file cá»§a báº¡n\n",
    "    batch_size = 2000\n",
    "    \n",
    "    # Check file exists\n",
    "    if not os.path.exists(json_file):\n",
    "        print(f\"âŒ File not found: {json_file}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Confirm upload\n",
    "    file_size_gb = os.path.getsize(json_file) / (1024**3)\n",
    "    print(f\"ğŸ“ File: {json_file} ({file_size_gb:.2f} GB)\")\n",
    "    \n",
    "    response = input(\"Proceed with upload? (y/N): \")\n",
    "    if response.lower() != 'y':\n",
    "        print(\"Cancelled.\")\n",
    "        exit()\n",
    "    \n",
    "    # Run upload\n",
    "    success = stream_json_and_upload(json_file, batch_size)\n",
    "    \n",
    "    if success:\n",
    "        # Test search\n",
    "        test_search_functionality()\n",
    "        print(\"\\nâœ… All done!\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Upload failed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
