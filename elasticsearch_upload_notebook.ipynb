{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elasticsearch configuration\n",
    "ES_HOST = \"http://localhost:9200\"\n",
    "INDEX_NAME = \"articles_content_title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticsearchUploader:\n",
    "    def __init__(self, es_host=ES_HOST):\n",
    "        self.es_host = es_host\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({'Content-Type': 'application/json'})\n",
    "        \n",
    "    def test_connection(self):\n",
    "        \"\"\"Test kết nối Elasticsearch\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.es_host}\")\n",
    "            if response.status_code == 200:\n",
    "                cluster_info = response.json()\n",
    "                print(f\" Connected to Elasticsearch {cluster_info['version']['number']}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\" Connection failed: {response.status_code}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\" Connection error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_index(self, index_name=INDEX_NAME):\n",
    "        \"\"\"Tạo index với mapping\"\"\"\n",
    "        \n",
    "        # Xóa index cũ nếu tồn tại\n",
    "        self.session.delete(f\"{self.es_host}/{index_name}\")\n",
    "        \n",
    "        mapping = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"article_id\": {\"type\": \"keyword\"},\n",
    "                    \"url\": {\"type\": \"keyword\"},\n",
    "                    \"title\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"standard\",\n",
    "                        \"fields\": {\n",
    "                            \"raw\": {\"type\": \"keyword\"},\n",
    "                            \"suggest\": {\"type\": \"completion\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"content\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"standard\"\n",
    "                    },\n",
    "                    \"date\": {\n",
    "                        \"type\": \"date\",\n",
    "                        \"format\": \"yyyy-MM-dd'T'HH:mm:ss'Z'||yyyy-MM-dd||epoch_millis\"\n",
    "                    },\n",
    "                    \"images\": {\"type\": \"keyword\"},\n",
    "                    \"created_at\": {\"type\": \"date\"},\n",
    "                    \"updated_at\": {\"type\": \"date\"}\n",
    "                }\n",
    "            },\n",
    "            \"settings\": {\n",
    "                \"number_of_shards\": 2,\n",
    "                \"number_of_replicas\": 1,\n",
    "                \"refresh_interval\": \"30s\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.put(\n",
    "                f\"{self.es_host}/{index_name}\",\n",
    "                data=json.dumps(mapping)\n",
    "            )\n",
    "            \n",
    "            if response.status_code in [200, 201]:\n",
    "                print(f\" Index '{index_name}' created successfully\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\" Failed to create index: {response.text}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Error creating index: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def bulk_upload(self, documents):\n",
    "        \"\"\"Upload documents bằng bulk API\"\"\"\n",
    "        \n",
    "        # Tạo bulk request body\n",
    "        bulk_body = \"\"\n",
    "        for doc in documents:\n",
    "            # Action line\n",
    "            action = {\n",
    "                \"index\": {\n",
    "                    \"_index\": doc[\"_index\"],\n",
    "                    \"_id\": doc[\"_id\"]\n",
    "                }\n",
    "            }\n",
    "            bulk_body += json.dumps(action) + \"\\n\"\n",
    "            \n",
    "            # Document line\n",
    "            bulk_body += json.dumps(doc[\"_source\"]) + \"\\n\"\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(\n",
    "                f\"{self.es_host}/_bulk\",\n",
    "                data=bulk_body,\n",
    "                headers={'Content-Type': 'application/x-ndjson'},\n",
    "                timeout=120\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                \n",
    "                # Đếm success và errors\n",
    "                success_count = 0\n",
    "                errors = []\n",
    "                \n",
    "                for item in result.get('items', []):\n",
    "                    if 'index' in item:\n",
    "                        if item['index'].get('status') in [200, 201]:\n",
    "                            success_count += 1\n",
    "                        else:\n",
    "                            errors.append(item['index'])\n",
    "                \n",
    "                return success_count, errors\n",
    "            else:\n",
    "                print(f\" Bulk upload failed: {response.status_code} - {response.text}\")\n",
    "                return 0, documents\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Bulk upload error: {e}\")\n",
    "            return 0, documents\n",
    "    \n",
    "    def search(self, query, size=10):\n",
    "        \"\"\"Search documents\"\"\"\n",
    "        search_body = {\n",
    "            \"query\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": query,\n",
    "                    \"fields\": [\"title^3\", \"content^1\"],\n",
    "                    \"type\": \"best_fields\",\n",
    "                    \"fuzziness\": \"AUTO\"\n",
    "                }\n",
    "            },\n",
    "            \"highlight\": {\n",
    "                \"fields\": {\n",
    "                    \"title\": {},\n",
    "                    \"content\": {\n",
    "                        \"fragment_size\": 150,\n",
    "                        \"number_of_fragments\": 3\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"size\": size\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(\n",
    "                f\"{self.es_host}/{INDEX_NAME}/_search\",\n",
    "                data=json.dumps(search_body)\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                print(f\" Search failed: {response.text}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Search error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_index_stats(self, index_name=INDEX_NAME):\n",
    "        \"\"\"Lấy thống kê index\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.es_host}/{index_name}/_stats\")\n",
    "            if response.status_code == 200:\n",
    "                stats = response.json()\n",
    "                doc_count = stats['indices'][index_name]['total']['docs']['count']\n",
    "                size_bytes = stats['indices'][index_name]['total']['store']['size_in_bytes']\n",
    "                return doc_count, size_bytes\n",
    "            else:\n",
    "                return 0, 0\n",
    "        except:\n",
    "            return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_json_and_upload(json_file_path, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Stream JSON file và upload bằng requests\n",
    "    \"\"\"\n",
    "    \n",
    "    # Khởi tạo uploader\n",
    "    uploader = ElasticsearchUploader()\n",
    "    \n",
    "    # Test connection\n",
    "    if not uploader.test_connection():\n",
    "        return False\n",
    "    \n",
    "    # Create index\n",
    "    if not uploader.create_index():\n",
    "        return False\n",
    "    \n",
    "    print(f\" Processing file: {json_file_path}\")\n",
    "    print(f\" File size: {os.path.getsize(json_file_path) / (1024**3):.2f} GB\")\n",
    "    print(f\" Batch size: {batch_size}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Counters\n",
    "    total_processed = 0\n",
    "    total_success = 0\n",
    "    total_errors = 0\n",
    "    batch = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        with open(json_file_path, 'rb') as file:\n",
    "            parser = ijson.kvitems(file, '')\n",
    "            \n",
    "            for article_id, article_data in parser:\n",
    "                try:\n",
    "                    # Validate\n",
    "                    if not isinstance(article_data, dict):\n",
    "                        continue\n",
    "                    \n",
    "                    # Create document\n",
    "                    doc = {\n",
    "                        \"_index\": INDEX_NAME,\n",
    "                        \"_id\": article_id,\n",
    "                        \"_source\": {\n",
    "                            \"article_id\": article_id,\n",
    "                            \"url\": article_data.get(\"url\", \"\"),\n",
    "                            \"title\": article_data.get(\"title\", \"\"),\n",
    "                            \"content\": article_data.get(\"content\", \"\"),\n",
    "                            \"date\": article_data.get(\"date\"),\n",
    "                            \"images\": article_data.get(\"images\", []),\n",
    "                            \"created_at\": datetime.now().isoformat(),\n",
    "                            \"updated_at\": datetime.now().isoformat()\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    batch.append(doc)\n",
    "                    \n",
    "                    # Upload batch khi đủ\n",
    "                    if len(batch) >= batch_size:\n",
    "                        success_count, errors = uploader.bulk_upload(batch)\n",
    "                        \n",
    "                        total_processed += len(batch)\n",
    "                        total_success += success_count\n",
    "                        total_errors += len(errors) if errors else 0\n",
    "                        \n",
    "                        # Progress update\n",
    "                        elapsed = time.time() - start_time\n",
    "                        speed = total_processed / elapsed if elapsed > 0 else 0\n",
    "                        \n",
    "                        print(f\" Processed: {total_processed:,} |  {total_success:,} |  {total_errors} |  {speed:.0f} docs/sec\")\n",
    "                        \n",
    "                        if errors:\n",
    "                            print(f\"    Last batch errors: {len(errors)}\")\n",
    "                        \n",
    "                        batch = []  # Reset batch\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\" Error processing {article_id}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Upload batch cuối\n",
    "            if batch:\n",
    "                success_count, errors = uploader.bulk_upload(batch)\n",
    "                total_processed += len(batch)\n",
    "                total_success += success_count\n",
    "                total_errors += len(errors) if errors else 0\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\" Error reading file: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Final stats\n",
    "    elapsed = time.time() - start_time\n",
    "    avg_speed = total_processed / elapsed if elapsed > 0 else 0\n",
    "    success_rate = (total_success / total_processed * 100) if total_processed > 0 else 0\n",
    "    \n",
    "    print(f\"\\n UPLOAD COMPLETED!\")\n",
    "    print(f\"⏱  Total time: {elapsed/60:.1f} minutes\")\n",
    "    print(f\" Documents processed: {total_processed:,}\")\n",
    "    print(f\" Successful: {total_success:,} ({success_rate:.1f}%)\")\n",
    "    print(f\" Errors: {total_errors}\")\n",
    "    print(f\" Average speed: {avg_speed:.0f} docs/sec\")\n",
    "    \n",
    "    # Refresh index\n",
    "    try:\n",
    "        uploader.session.post(f\"{uploader.es_host}/{INDEX_NAME}/_refresh\")\n",
    "        print(\" Index refreshed\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Verify upload\n",
    "    doc_count, size_bytes = uploader.get_index_stats()\n",
    "    print(f\" Final index stats: {doc_count:,} docs, {size_bytes/(1024**2):.1f} MB\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def test_search_functionality():\n",
    "    \"\"\"Test search sau khi upload\"\"\"\n",
    "    uploader = ElasticsearchUploader()\n",
    "    \n",
    "    print(\"\\n TESTING SEARCH\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Test search\n",
    "    results = uploader.search(\"TuSimple autonomous truck\", size=3)\n",
    "    \n",
    "    if results and results['hits']['total']['value'] > 0:\n",
    "        print(f\" Found {results['hits']['total']['value']} results\")\n",
    "        \n",
    "        for i, hit in enumerate(results['hits']['hits'], 1):\n",
    "            source = hit['_source']\n",
    "            print(f\"\\n{i}. {source['title']}\")\n",
    "            print(f\"   Score: {hit['_score']:.2f}\")\n",
    "            print(f\"   URL: {source['url']}\")\n",
    "            \n",
    "            # Highlight\n",
    "            if 'highlight' in hit and 'content' in hit['highlight']:\n",
    "                print(f\"   Preview: {hit['highlight']['content'][0]}...\")\n",
    "    else:\n",
    "        print(\" No search results found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ELASTICSEARCH UPLOADER WITH REQUESTS\n",
      "==================================================\n",
      " File: database_article_to_alls.json (1.01 GB)\n",
      " Connected to Elasticsearch 9.0.1\n",
      " Index 'articles_content_title' created successfully\n",
      " Processing file: database_article_to_alls.json\n",
      " File size: 1.01 GB\n",
      " Batch size: 2000\n",
      "--------------------------------------------------\n",
      " Processed: 2,000 |  2,000 |  0 |  427 docs/sec\n",
      " Processed: 4,000 |  4,000 |  0 |  448 docs/sec\n",
      " Processed: 6,000 |  6,000 |  0 |  434 docs/sec\n",
      " Processed: 8,000 |  8,000 |  0 |  495 docs/sec\n",
      " Processed: 10,000 |  10,000 |  0 |  487 docs/sec\n",
      " Processed: 12,000 |  12,000 |  0 |  503 docs/sec\n",
      " Processed: 14,000 |  14,000 |  0 |  503 docs/sec\n",
      " Processed: 16,000 |  16,000 |  0 |  487 docs/sec\n",
      " Processed: 18,000 |  18,000 |  0 |  493 docs/sec\n",
      " Processed: 20,000 |  20,000 |  0 |  495 docs/sec\n",
      " Processed: 22,000 |  22,000 |  0 |  509 docs/sec\n",
      " Processed: 24,000 |  24,000 |  0 |  518 docs/sec\n",
      " Processed: 26,000 |  26,000 |  0 |  527 docs/sec\n",
      " Processed: 28,000 |  28,000 |  0 |  533 docs/sec\n",
      " Processed: 30,000 |  30,000 |  0 |  539 docs/sec\n",
      " Processed: 32,000 |  32,000 |  0 |  547 docs/sec\n",
      " Processed: 34,000 |  34,000 |  0 |  556 docs/sec\n",
      " Processed: 36,000 |  36,000 |  0 |  560 docs/sec\n",
      " Processed: 38,000 |  38,000 |  0 |  563 docs/sec\n",
      " Processed: 40,000 |  40,000 |  0 |  565 docs/sec\n",
      " Processed: 42,000 |  42,000 |  0 |  570 docs/sec\n",
      " Processed: 44,000 |  44,000 |  0 |  574 docs/sec\n",
      " Processed: 46,000 |  46,000 |  0 |  579 docs/sec\n",
      " Processed: 48,000 |  48,000 |  0 |  582 docs/sec\n",
      " Processed: 50,000 |  50,000 |  0 |  584 docs/sec\n",
      " Processed: 52,000 |  52,000 |  0 |  581 docs/sec\n",
      " Processed: 54,000 |  54,000 |  0 |  579 docs/sec\n",
      " Processed: 56,000 |  56,000 |  0 |  574 docs/sec\n",
      " Processed: 58,000 |  58,000 |  0 |  567 docs/sec\n",
      " Processed: 60,000 |  60,000 |  0 |  565 docs/sec\n",
      " Processed: 62,000 |  62,000 |  0 |  564 docs/sec\n",
      " Processed: 64,000 |  64,000 |  0 |  566 docs/sec\n",
      " Processed: 66,000 |  66,000 |  0 |  570 docs/sec\n",
      " Processed: 68,000 |  68,000 |  0 |  569 docs/sec\n",
      " Processed: 70,000 |  70,000 |  0 |  566 docs/sec\n",
      " Processed: 72,000 |  72,000 |  0 |  563 docs/sec\n",
      " Processed: 74,000 |  74,000 |  0 |  557 docs/sec\n",
      " Processed: 76,000 |  76,000 |  0 |  553 docs/sec\n",
      " Processed: 78,000 |  78,000 |  0 |  554 docs/sec\n",
      " Processed: 80,000 |  80,000 |  0 |  554 docs/sec\n",
      " Processed: 82,000 |  82,000 |  0 |  556 docs/sec\n",
      " Processed: 84,000 |  84,000 |  0 |  557 docs/sec\n",
      " Processed: 86,000 |  86,000 |  0 |  560 docs/sec\n",
      " Processed: 88,000 |  88,000 |  0 |  562 docs/sec\n",
      " Processed: 90,000 |  90,000 |  0 |  563 docs/sec\n",
      " Processed: 92,000 |  92,000 |  0 |  562 docs/sec\n",
      " Processed: 94,000 |  94,000 |  0 |  565 docs/sec\n",
      " Processed: 96,000 |  96,000 |  0 |  564 docs/sec\n",
      " Processed: 98,000 |  98,000 |  0 |  565 docs/sec\n",
      " Processed: 100,000 |  100,000 |  0 |  565 docs/sec\n",
      " Processed: 102,000 |  102,000 |  0 |  564 docs/sec\n",
      " Processed: 104,000 |  104,000 |  0 |  563 docs/sec\n",
      " Processed: 106,000 |  106,000 |  0 |  564 docs/sec\n",
      " Processed: 108,000 |  108,000 |  0 |  566 docs/sec\n",
      " Processed: 110,000 |  110,000 |  0 |  569 docs/sec\n",
      " Processed: 112,000 |  112,000 |  0 |  570 docs/sec\n",
      " Processed: 114,000 |  114,000 |  0 |  570 docs/sec\n",
      " Processed: 116,000 |  116,000 |  0 |  570 docs/sec\n",
      " Processed: 118,000 |  118,000 |  0 |  572 docs/sec\n",
      " Processed: 120,000 |  120,000 |  0 |  574 docs/sec\n",
      " Processed: 122,000 |  122,000 |  0 |  576 docs/sec\n",
      " Processed: 124,000 |  124,000 |  0 |  576 docs/sec\n",
      " Processed: 126,000 |  126,000 |  0 |  576 docs/sec\n",
      " Processed: 128,000 |  128,000 |  0 |  576 docs/sec\n",
      " Processed: 130,000 |  130,000 |  0 |  577 docs/sec\n",
      " Processed: 132,000 |  132,000 |  0 |  579 docs/sec\n",
      " Processed: 134,000 |  134,000 |  0 |  581 docs/sec\n",
      " Processed: 136,000 |  136,000 |  0 |  583 docs/sec\n",
      " Processed: 138,000 |  138,000 |  0 |  584 docs/sec\n",
      " Processed: 140,000 |  140,000 |  0 |  585 docs/sec\n",
      " Processed: 142,000 |  142,000 |  0 |  585 docs/sec\n",
      " Processed: 144,000 |  144,000 |  0 |  585 docs/sec\n",
      " Processed: 146,000 |  146,000 |  0 |  587 docs/sec\n",
      " Processed: 148,000 |  148,000 |  0 |  588 docs/sec\n",
      " Processed: 150,000 |  150,000 |  0 |  588 docs/sec\n",
      " Processed: 152,000 |  152,000 |  0 |  588 docs/sec\n",
      " Processed: 154,000 |  154,000 |  0 |  589 docs/sec\n",
      " Processed: 156,000 |  156,000 |  0 |  589 docs/sec\n",
      " Processed: 158,000 |  158,000 |  0 |  588 docs/sec\n",
      " Processed: 160,000 |  160,000 |  0 |  588 docs/sec\n",
      " Processed: 162,000 |  162,000 |  0 |  587 docs/sec\n",
      " Processed: 164,000 |  164,000 |  0 |  586 docs/sec\n",
      " Processed: 166,000 |  166,000 |  0 |  586 docs/sec\n",
      " Processed: 168,000 |  168,000 |  0 |  583 docs/sec\n",
      " Processed: 170,000 |  170,000 |  0 |  581 docs/sec\n",
      " Processed: 172,000 |  172,000 |  0 |  581 docs/sec\n",
      " Processed: 174,000 |  174,000 |  0 |  582 docs/sec\n",
      " Processed: 176,000 |  176,000 |  0 |  581 docs/sec\n",
      " Processed: 178,000 |  178,000 |  0 |  581 docs/sec\n",
      " Processed: 180,000 |  180,000 |  0 |  581 docs/sec\n",
      " Processed: 182,000 |  182,000 |  0 |  581 docs/sec\n",
      " Processed: 184,000 |  184,000 |  0 |  582 docs/sec\n",
      " Processed: 186,000 |  186,000 |  0 |  581 docs/sec\n",
      " Processed: 188,000 |  188,000 |  0 |  580 docs/sec\n",
      " Processed: 190,000 |  190,000 |  0 |  578 docs/sec\n",
      " Processed: 192,000 |  192,000 |  0 |  577 docs/sec\n",
      " Processed: 194,000 |  194,000 |  0 |  577 docs/sec\n",
      " Processed: 196,000 |  196,000 |  0 |  577 docs/sec\n",
      " Processed: 198,000 |  198,000 |  0 |  578 docs/sec\n",
      " Processed: 200,000 |  200,000 |  0 |  579 docs/sec\n",
      " Processed: 202,000 |  202,000 |  0 |  580 docs/sec\n",
      "\n",
      " UPLOAD COMPLETED!\n",
      "⏱  Total time: 5.8 minutes\n",
      " Documents processed: 202,803\n",
      " Successful: 202,803 (100.0%)\n",
      " Errors: 0\n",
      " Average speed: 582 docs/sec\n",
      " Index refreshed\n",
      " Final index stats: 202,803 docs, 1170.2 MB\n",
      "\n",
      " TESTING SEARCH\n",
      "------------------------------\n",
      " Found 10000 results\n",
      "\n",
      "1. Roboracer autonomous car to make history at Goodwood\n",
      "   Score: 34.34\n",
      "   URL: https://www.cnn.com/2018/06/28/sport/roboracer-goodwood-festival-of-speed-spt-intl/index.html\n",
      "   Preview: It is a venerable shrine to the thrills of the internal combustion engine, but Goodwood is famous Festival of Speed will welcome an <em>autonomous</em> electric...\n",
      "\n",
      "2. AI’s ‘Oppenheimer moment’: autonomous weapons enter the battlefield\n",
      "   Score: 34.34\n",
      "   URL: https://www.theguardian.com/technology/article/2024/jul/14/ais-oppenheimer-moment-autonomous-weapons-enter-the-battlefield\n",
      "   Preview: One of them makes a call over his radio, and within moments a fleet of small <em>autonomous</em> drones equipped with explosives fly through the town square, entering...\n",
      "\n",
      "3. Robot surgeons provide many benefits, but how autonomous should they be?\n",
      "   Score: 33.98\n",
      "   URL: https://www.theguardian.com/science/2023/jun/18/robot-surgeons-provide-many-benefits-but-how-autonomous-should-they-be\n",
      "   Preview: In what they described as one of the most delicate procedures in the practice of surgery, their Smart Tissue <em>Autonomous</em> Robot (Star) sutured the ends of...\n",
      "\n",
      " All done!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\" ELASTICSEARCH UPLOADER WITH REQUESTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Configuration\n",
    "    json_file = \"database_article_to_alls.json\"  # Thay đường dẫn file của bạn\n",
    "    batch_size = 2000\n",
    "    \n",
    "    # Check file exists\n",
    "    if not os.path.exists(json_file):\n",
    "        print(f\" File not found: {json_file}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Confirm upload\n",
    "    file_size_gb = os.path.getsize(json_file) / (1024**3)\n",
    "    print(f\" File: {json_file} ({file_size_gb:.2f} GB)\")\n",
    "    \n",
    "    response = input(\"Proceed with upload? (y/N): \")\n",
    "    if response.lower() != 'y':\n",
    "        print(\"Cancelled.\")\n",
    "        exit()\n",
    "    \n",
    "    # Run upload\n",
    "    success = stream_json_and_upload(json_file, batch_size)\n",
    "    \n",
    "    if success:\n",
    "        # Test search\n",
    "        test_search_functionality()\n",
    "        print(\"\\n All done!\")\n",
    "    else:\n",
    "        print(\"\\n Upload failed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
